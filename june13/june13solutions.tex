\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, mathrsfs}
\usepackage[left=1.0in,right=1.0in,top=1.0in,bottom=1.0in]{geometry}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{times}
\usepackage{xcolor}

\newcommand{\R}{\mathbf{R}}
\newcommand{\BB}[1]{\textcolor{blue}{#1}}

\begin{document}

\begin{center}
  {\Large\bfseries Math 240 Tutorial \\ Questions}
\end{center}
\begin{center}
  {\bfseries June 13}
\end{center}

\noindent{\bfseries Question 1.} Show the following for $\R^n$.
\begin{enumerate}[(a)]
\item Show that scalar multiplication is a linear transformation. \\

  \BB{Fix $a \in \R$, and let $T:\R^n \rightarrow \R^n$ be the map $T(\vec
    v)=a\vec v$. Then $T(b\vec v)=ab\vec v=ba\vec v=bT(\vec v)$ and $T(\vec
    v+\vec u)=a(\vec v+\vec u)=a\vec v+a\vec u=T(\vec v)+T(\vec u)$. We have
    shown that $T$ is linear. \\}

\item When is this linear map invertible? \\

  \BB{This map is invertible precisely in the case the scalar by which we are
    multiplying is nonzero. \\}
  
\item Is its inverse a linear transformation? \\

  \BB{Let $T$ be as in part (a), and assume that $a \neq 0$. Then $T$ is
    invertible and $T^{-1}$ is given by multiplcation by $a^{-1}$. Since this
    is multiplcation by a scalar, it is linear. \\}
  
\item Fix an element $a \in \R^n$. What is the matrix corresponding to the
linear transformation $\vec v \mapsto a\vec v$ with respect to the standard
spanning vectors? \\

\BB{Recall the standard spanning vectors are $\{\vec e_1,\,\vec e_2,\dots,\,\vec
  e_n\}$ where $\vec e_i$ is the vector with a 1 in position $i$ and zeros
  everywhere else. Then the matrix corresponding to multiplication by $a$ is
  givn by
  \[
    [T(\vec e_1) | T(\vec e_2) | \cdots | T(\vec e_n)]=
    [a\vec e_1 | a\vec e_2 | \cdots | a\vec e_n] = aI.
  \]
}
\end{enumerate}

\noindent{\bfseries Question 2.} Give the matrix for the transformation that
rotates vectors in $\R^2$ by $2\pi/3$ radians. \\

\BB{We consider the standard basis vectors $e_1=(1,0)$ and $e_2=(0,1)$. The
  vector $e_1$ maps to $(-1/2,\sqrt{3}/2)$, and the vector $e_2$ maps to
  $(-1/2,-\sqrt{3}/2)$. So the matrix for the transformation is given by
  \[
    \left(
      \begin{array}{cc}
        -\frac{1}{2} & \frac{\sqrt{3}}{2} \\
        -\frac{1}{2} & -\frac{\sqrt{3}}{2} \\
      \end{array}
    \right).
  \] \\
}

\noindent{\bfseries Question 3.} Fix $a \in \R$ and $\vec u \in \R^n$ with $\vec
u \neq \vec 0$. Is the map given by $\vec v \mapsto a\vec v + \vec u$, linear?
Why or why not? \\

\BB{No; it is not a linear map. Let $\vec v_1,\,\vec v_2 \in \R^n$. Then $\vec
  v_1+\vec v_2 \mapsto a\vec v_1+a\vec v_2+\vec u$. But $\vec v_1 \mapsto a\vec
  v_1+\vec u$ and $\vec v_2 \mapsto a\vec v_2+\vec u$. However, $(a\vec v_1+\vec
  u)+(a\vec v_2+\vec u)=a\vec v_1+a\vec v_2+2\vec u \neq a\vec v_1+a\vec
  v_2+\vec u$. \\}

\noindent{\bfseries Question 4.} Consider a linear transformation $T: \R^n
\rightarrow \R^n$, and define $\text{Ker}(T)=\{\vec v \in \R^n : T(\vec v)=\vec
0\}$. This is the kernel of the linear transformation $T$. For $\vec v \in
\R^n$, define $\vec v + \text{Ker}(T)=\{\vec v + \vec u : \vec u \in
\text{Ker}(T)\}$. Show the following.
\begin{enumerate}[(a)]
\item $\text{Ker}(T)$ is closed under scalar multiplcation and vector addition.\\

  \BB{Let $\vec v_1,\,\vec v_2 \in \text{Ker}(T)$. Then $T(\vec v_1+\vec
    v_2)=T(\vec v_1)+T(\vec v_2)=\vec 0+\vec 0=\vec 0$, so $\vec v_1+\vec v_2
    \in \text{Ker}(T)$. Similarly, $T(a\vec v)=aT(\vec v)=a\vec 0=\vec 0$
    whenever $\vec v \in \text{Ker}(T)$; so, $a\vec v \in \text{Ker}(T)$. This
    shows that $\text{Ker}(T)$ is closed under vector addition and scalar
    multiplication. \\}
  
\item For $\vec v \in \R^n$, show that $\vec v + \text{Ker}(T)$ consists of all
  and only those elements of $\R^n$ that map to $T(\vec v)$ under $T$. \\

  \BB{Let $V=\{u \in \R^n : T(\vec u)=T(\vec v)\}$. We show $\vec
    v+\text{Ker}(T)=V$. Indeed, let $\vec u \in V$. Then $T(\vec v)=T(\vec u)$
    so that $T(u-v)=\vec 0$. It follows that there is an $\vec x \in
    \text{Ker}(T)$ such that $\vec u-\vec v=\vec x$, that is, $\vec u=\vec
    v+\vec x$. This means $\vec u \in \vec v+\text{Ker}(T)$.}

  \BB{Conversely, let $\vec u \in \vec v+\text{Ker}(T)$. Then there is some
    $\vec x \in \text{Ker}(T)$ for which $\vec u=\vec v+\vec x.$ It follows that
  $T(\vec u)=T(\vec v+\vec x)+T(\vec v)+T(\vec x)=T(\vec v)+\vec 0=T(\vec v)$.
  So, $\vec u \in V$, as required. \\}

\item For $\vec v_1,\vec v_2 \in \R^n$, show that either $\vec
  v_1+\text{Ker}(T)=\vec v_2 +\text{Ker}(T)$ or $\vec v_1+\text{Ker}(T) \cap
  \vec v_2 +\text{Ker}(T)=\emptyset$. \\

  \BB{Let $\vec v_1,\,\vec v_2 \in \R^n$, and suppose that $\vec
    v_1+\text{Ker}(T) \cap \vec v_2+\text{Ker}(T) \neq \emptyset$. Then there is
    some $\vec u \in v_1+\text{Ker}(T) \cap \vec v_2+\text{Ker}(T)$. By
    definition, there must be $\vec u_1,\,\vec u_2 \in \text{Ker}(T)$ such that
    $\vec u=\vec v_1+\vec u_1=\vec v_2=\vec u_2$. But then $\vec v_1-\vec
    v_2=\vec u_2-\vec u_1=\vec u_3$ for some $\vec u_3 \in \text{Ker}(T)$. It
    follows that $\vec v_1=\vec v_2+\vec u_3$, so $\vec v_1 \in \vec
    v_2+\text{Ker}(T)$. Similarly, $\vec v_2=\vec v_1-\vec u_3$, so $\vec v_2 \in
    \vec v_1+\text{Ker}(T)$. It follows, therefore, that $\vec
    v_1+\text{Ker}(T)=\vec v_2+\text{Ker}(T)$.\\}
\end{enumerate}

\noindent{\bfseries Question 5.} Find a matrix $A$ such that $A^4=O$, but no
smaller positive power $A$ is $O$. \\

\BB{The matrix
  \[
    A=\begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0
      \end{pmatrix}
  \]
  satisfies the stated requirements.\\}

\noindent{\bfseries Question 6.} The trace of a square matrix $A$ of dimensions
$N \times N$ is defined as $\text{tr}(A)=\sum_{k=1}^NA_{k,k}$, i.e., the sum of
the diagonal entries of the matrix. For any other $N \times N$ matrix $B$, show
that $\text{tr}(AB)=\text{tr}(BA)$. \\

\BB{Observe
  \begin{align*}
    \text{tr}(AB) &= \sum_{k=1}^N(AB)_{k,k} \\
                  &= \sum_{k=1}^N\sum_{j=1}^NA_{k,j}B_{j,k} \\
                  &= \sum_{k=1}^N\sum_{j=1}^NA_{j,k}B_{k,j} \\
                  &= \text{tr}(BA)
  \end{align*}
  where the second to last equality follows because
  \[
    \{(k,j,j,k) : 1 \leqq j,k \leqq N\} =
    \{(j,k,k,j) : 1 \leqq j,k \leqq N\}.
  \] \\
}

\noindent{\bfseries Question 7.} An $N \times N$ matrix $A$ is circulant if it is
of the form
\[
  A=\begin{pmatrix}
      a_1 & a_2 & a_3 & \cdots & a_N \\
      a_N & a_1 & a_2 & \cdots & a_{N-1} \\
      a_{N-1} & a_N & a_1 & \cdots & a_{N-2} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      a_2 & a_3 & a_4 & \cdots & a_1
    \end{pmatrix}.
\]
Show that if $B$ is any other $N \times N$ circulant matrix, then $AB=BA$. \\

\BB{Define the $N \times N$ matrix $G$ by
  \[
    G=\begin{pmatrix}
        0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \cdots & \vdots \\
        1 & 0 & 0 & \cdots & 0
      \end{pmatrix}.
    \]
    If we multiply an $N \times N$ matrix $C$ on the right by $G$, the resulting
    matrix is the one obtained by cyclically shifting the columns of $C$ to the
    right. In particular, $G^N=I$ and $G^j \neq I$ for any $j \in
    \{1,\dots,N-1\}$. We also note that we can write $A$ and $B$ by
    \[
      A = \sum_{i=1}^Na_iG^{i-1}, \qquad B = \sum_{i=1}^Nb_iG^{i-1},
    \]
    that is, $A$ and $B$ are polynomials in $G$. Since they are each polynomials
    in $G$, it is easy to see that they must commute. \\}

\noindent{\bfseries Question 8.} Let $N=\{1,2,\dots,n\}$. A permutation of $N$
is an invertible map $N \rightarrow N$. Write the $n \times n$ identity matrix
as
\[
  I=[e_1 \mid e_2 \mid \cdots \mid e_n],
\]
and let $\sigma$ be a permutation of $N$. The matrix corresponding to $\sigma$
is given by
\[
  P_\sigma = [e_{\sigma(1)} \mid e_{\sigma(2)} \mid \cdots \mid e_{\sigma(n)}].
\]
Answer the following.
\begin{enumerate}[(a)]
\item Derive an expression for the $(i,j)$ entry of $P_\sigma$. \\

  \BB{Recall the so-called Kronecker delta function defined by
    \[
      \delta_{i,j}=\begin{cases}
                     1 & \text{if $i=j$,} \\
                     0 & \text{if $i \neq j$.}
                   \end{cases}
    \]
    By definition, the $(i,j)$ entry of $P_\sigma$ is 1 exactly in the case that
    $i=\sigma^{-1}(j)$. So, we write $P_{\sigma_{i,j}}=\delta_{i,\sigma^{-1}j}$. \\
  }
  
\item If $A$ is any other $n \times n$ matrix, what effect does doing the
  multiplcation $AP_\sigma$ have? \\

  \BB{By definition, we have
    \[
      AP_\sigma = A[e_{\sigma(1)} \mid e_{\sigma(2)} \mid \cdots \mid e_{\sigma(n)}]
      = [Ae_{\sigma(1)} \mid Ae_{\sigma(2)} \mid \cdots \mid Ae_{\sigma(n)}],
    \]
    but $Ae_j$ is simply the $j$-th column of $A$; so, the effect of multiplying
    on the right by $P_\sigma$ is simply to apply the permutation $\sigma$ to
    the columns of $A$.}

  \BB{We could also infer this from part (a). Observe
    \[
      (AP_\sigma)_{i,j} = \sum_kA_{i,k}P_{\sigma_{k,j}} = A_{i,\sigma^{-1}(j)}.
    \]
  }
  
\item If $B$ is any other $n \times n$ matrix, what effect does doing the
  multiplcation $P_\sigma B$ have? \\

  \BB{Here, we need to employ part (a). We have
    \[
      (P_{\sigma}B)_{i,j}=\sum_kP_{\sigma_{i,k}}B_{k,j}=B_{\sigma(i),j}.
    \]
    This means that the permutation $\sigma^{-1}$ is applied to the rows of $B$.\\}
  
\item Is $P_\sigma$ invertible? If it is, what is its inverse? \\

  \BB{Yes; it is invertible. The inverse of $P_\sigma$ is given by
    $P_{\sigma^{-1}}$. From part (a), we note that the $(i,j)$ entry of
    $P_{\sigma}^t$ is $\delta_{j,\sigma^{-1}(i)}=\delta_{i,\sigma(j)}$, which is
  applying the permutation $\sigma^{-1}$ to the columns of $I$. We have shown
  $P_\sigma^{-1}=P_{\sigma^{-1}}=P_\sigma^t$.\\}
  
\item How many columns(rows) are fixed by $P_\sigma$. \\

  \BB{If we are multiypling by $P_\sigma$ on the right, then we are interested
    in the number of fixed columns. This is equal to the number of indices $i$
    such that $\sigma(i)=i$ which implies $e_{\sigma(i)}=e_i$. Note that this is
  the same as counting the number of 1s along the diagonal. Since the entries of
$P_\sigma$ different than 1 are 0, it follows that the number of columns fixed
by $P_\sigma$ is given by $\text{tr}(P_\sigma)$. For this reason,
$\text{tr}(P_\sigma)$ is often called the permutation character of $P_\sigma$.\\}

\end{enumerate}

\noindent{\bfseries Question 9.} A diagonal matrix is one for which every entry
not on the main diagonal is zero. Let $A$ and $B$ be $N \times N$ matrices such
that there exists and invertible $N \times N$ matrix $P$ for which
$D_A=P^{-1}AP$ and $D_B=P^{-1}BP$ are diagonal matrices. Show that $A$ and $B$
commute. \\

\BB{Since $D_A=P^{-1}AP$ and $D_B=P^{-1}BP$, we have that $A=PD_AP^{-1}$ and
  $B=PD_BP^{-1}$. Then
  \begin{align*}
    AB &= (PD_AP^{-1})(PD_BP^{-1}) \\
       &= PD_A(P^{-1}P)D_BP^{-1} \\
       &= PD_AID_BP^{-1} \\
       &= PD_AD_BP^{-1} \\
       &= PD_BD_AP^{-1} \\
       &= PD_BID_AP^{-1} \\
       &= PD_B(P^{-1}P)D_AP^{-1} \\
       &= (PD_BP^{-1})(PD_AP^{-1}) \\
       &= BA
  \end{align*}
  where we have used the fact that diagonal matrices commute.
}

\end{document}